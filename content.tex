\npsection{Introduction}

ML: Tries to automate the process of \bx{inductive inference}.

\be
\i Deduction: Learning from rules
\i Induction: Learning from examples
\ee

\subsection{Math}

\TODO{norms}
\TODO{determinant, trace, inverse}
\TODO{semidefinite, definite, indefinite}
\TODO{linear eq}
\TODO{inverse proof}

\subsubsection{Eigenvalues and Eigenvectors}

Example: $f(w) = 0.5 w^TMw$
\bi
\i Hessian: $\nabla^2 f(w) = M = \vct{1 & 0 \\ 0 & 3}$
\i Eigenvalues $1,3$
\i Function along eigenvectors like $1x^2$ and $3x^2$
\ei

\subsubsection{Derivative and Hessian}

\newcommand{\dlw}[2]{\frac{\partial L_#1}{\partial w_#2}}
\begin{align*}
L(w): \RS \to \RS^m
\Rightarrow  \nabla L(w) = \vct{\drv{w_1}L(w)\\\drv{w_2}L(w)\\\vdots\\\drv{w_n}L(w)}
\Rightarrow \nabla L(w) = \left(\begin{array}{cccc}
\dlw{1}{1} & \dlw{2}{1} & \ldots & \dlw{m}{1} \\
\dlw{1}{2} & \dlw{2}{2} & \ldots & \dlw{m}{2} \\
\vdots & \vdots & \ddots & \vdots \\
\dlw{1}{d} & \dlw{2}{d} & \ldots & \dlw{m}{d}
\end{array}\right)
\end{align*}

\newcommand{\dfdx}[2]{
    \frac{\partial^2 f}{\partial x_{#1} \partial x_{#2}} (x)
}
\begin{align*}
\nabla^2 f(x) =
\vct{
    \dfdx{1}{1} & \ldots & \dfdx{1}{d} \\
    \vdots & \ddots & \vdots \\
    \dfdx{d}{1} & \ldots & \dfdx{d}{d} 
}, \qquad x = \vct{x_1\\\vdots\\x_d} \in \RS^d
\end{align*}

\subsubsection{Bonus}

Convex hull $\conv(V)$ for set of vectors $V$ is smallest convex set containing $V$.
\[ \conv(V) = \left\{\dsum^m_{i=1} \lambda_i \cdot v_i \mid \lambda_i \geq 0, \dsum^m_{i=1} \lambda_i = 1\right\} \]

\npsection{Supervised learning}

\bi
\i input $X$, output $Y$
\i training data: $(x^{(i)}, y^{(i)})_{i=1..n} \subset X \times Y$
\i Goal: learn $f: X \to Y$ for model class $F$ on examples
\ei

\subsection{Least squares regression}
$\tilde X, \tilde w$ are extended with bias:
\begin{align*}
\min_{\tilde w} \frac{1}{2} \norm{\tilde X \tilde w - y}^2 \Rightarrow \min_w \frac{1}{2} \norm{Xw-y}^2
\end{align*}
Solve with gradient and set to zero:
\begin{align*}
L &= \frac{1}{2} \dsum_{i=1}^n ((X_i^Tw_i)-y_i)^2 \\
&= \frac{1}{2} \left(\dsum_{i=1}^n (X_i^Tw_i)^2 - 2(X_i^Tw_i)y_i + y_i^2 \right) \\
\end{align*}
\begin{align*}
  \nabla L &= \drv{w} \left(  \frac{1}{2} \left(\dsum_{i=1}^n (X_i^Tw_i)^2 - 2(X_i^Tw_i)y_i + y_i^2 \right) \right)\\
  &= \frac{1}{2} \left(\dsum_{i=1}^n 2(X_i^TX_iw_i) - 2(X_i^T)y_i \right)\\
  &= \dsum_{i=1}^n X_i^TX_iw_i - X_i^Ty_i\\
  &= X^TXw - X^Ty\\
  &= X^T(Xw-y)
\end{align*}
\begin{align*}
\nabla L = X^T(Xw-y) = 0 \Rightarrow (X^TX)w = X^Ty \Rightarrow w = (X^TX)^{-1}X^Ty
\end{align*}


\subsection{Error types}

\bi
\i real value $y^{(i)}$, predicted value $\hat y^{(i)}$
\i Mean squared error is average error, applied to any regression model: \[\frac{1}{n} \dsum_{i=1}^n (y^{(i)} - \hat y^{(i)})^2\]
\i Mean absolute error is average absolute error: \[\frac{1}{n} \dsum_{i=1}^n \left|y^{(i)} - \hat y^{(i)}\right|\]
\i Root mean squared error: \[\sqrt{\frac{1}{n} \dsum_{i=1}^n (y^{(i)} - \hat y^{(i)})^2}\]
\ei

\subsection{Precision, Recall, etc.}

\bi
\i Precision: $\frac{TP}{TP + FP}$, how many of predicted positive are actually positive
\i Recall (true positive rate, sensitivity): $\frac{TP}{TP + FN}$, how many of actual positive are predicted positive
\i TN rate (specificity): $\frac{TN}{TN + FP}$, how many of actual negative are predicted negative
\i Accuracy (ACC): $\frac{TP + TN}{TP + TN + FP + FN}$
\i Balanced accuracy: $\frac{1}{2} \left( \frac{TP}{TP + FN} + \frac{TN}{TN + FP} \right)$
\i F1 score (ignores usually large true negatives): $2 \cdot \frac{\text{Precision} \cdot \text{Recall}}{\text{Precision} + \text{Recall}}$ 
\ei

\subsection{Gradient descent}
Alternative to least squares regression. Algorithm:
\be
\i Compute gradient $\nabla L(w) = X^T(Xw-y)$
\i Negative gradient shows to steepest descent
\i $w^{(t+1)} = w^{(t)} - \gamma^{(t)} \cdot \nabla L(w^{(t)})$
\ee

\subsubsection{Derivative examples}

\bi
\i $L(w) = w_1^2 + w_2^2 \\\Rightarrow \nabla L(w) = \vct{2w_1\\2w_2}$
\i $L(w) = \norm{w} ^2_2 = w^Tw \\\Rightarrow \nabla L(w) = 2w$
\i $L(w) = w^TAw \\\Rightarrow \nabla L(w) = Aw + A^Tw$
\i $L(w) = \norm{Xw-y}^2 = w^TX^TXw - y^TXw - w^TX^Ty + y^Ty \\\Rightarrow \nabla L(w) = 2X^T(Xw - y)$
\ei

\subsubsection{Convexity}

Set $C$ convex if line between any two points of $C$ in $C$. $\forall x,y \in C$ and $\lambda \in \RS$ with $0 \leq \lambda \leq 1$: \[\lambda x + (1 - \lambda) y \in C\]

Function $f: \RS^d \to \RS$ convex if $\dom(f)$ is a convex set and $\forall x,y \in \dom(f)$, $\lambda \in \RS$ with $0 \leq \lambda \leq 1$: \[f(\lambda x + (1 - \lambda) y) \leq \lambda f(x) + (1 - \lambda) f(y)\]

Gradient descent returns global optimum for convex functions.

Optimization problem: $\min f(x), x \in X \subseteq \RS^d$ has local minimizer $x^* \in X$ if $\exists \epsilon > 0$ with:
\[\forall y \in X \text{ with } \norm{x^* - y} \leq \epsilon: f(x^*) \leq f(y)\]
Global minimizer if $f(x^*)$ is lowest of all optimizers.

Symmetric matrix $A$ is positive semidefinite ($A \succcurlyeq 0$) if :
\[x^TAx \geq 0, \forall x\]
Positive definite ($A \succ 0$) if $\forall x \neq 0$

Symmetric matrix $A$ is positive semidefinite iff all eigenvalues are $\geq 0$ and positive definite iff all $> 0$.

If function is one-dimensional: Convex if $f''(x) \geq 0$.
If multidimensional: Convex if 2nd derivative is psd.


\subsubsection{Backtracking line search} % not needed in exam

Algorithm:
\be
\i Input: $x, \Delta x, \alpha \in (0,0.5), \beta \in (0,1)$
\i $t = 1$
\i while $f(x + t \, \Delta x) > f(x) + \alpha \, t \, \nabla f(x)^T \, \Delta x$:
\i \quad $t = \beta \, t$
\ee

\subsubsection{Solve LSR}

\be
\i $L(w) = \frac{1}{2} \norm{Xw-y}_2^2$
\i $\nabla L(w) = X^T(Xw-y)$
\i $\nabla L(w) = X^TX$ is symmetric and psd
\ee

\subsubsection{Subgradient method} % not needed in exam

If function not differentiable, e.g. $\norm{w}_1$
\bi
\i gradient is subgradient (convex hull of gradients)
\i choose constant step length $g$
\i $w^{(t+1)} = w^{(t)} - \gamma^{(t)} \cdot g$ with $\gamma^{(t)} = \frac{1}{\sqrt{t}}$
\i find $g \in \RS^d$ at $x \in \dom(f)$ with: \[f(y) \geq f(x) + g^T(y-x), \forall y \in \dom(f)\]
\ei

\subsection{Polynomial Regression}

\bi
\i $X \in \RS, Y \in \RS$
\i $f(x) = w_dx^d + w_{d-1}x^{d-1} + \ldots + w_1x^1 + w_0$
\i find best $w = (w_d, \ldots, w_0) \in \RS^{d+1}$
\i loss function is squared loss: $l(y, \hat y) = \frac{1}{2}(y - \hat y)^2$
\ei

\begin{samepage}
With $\hat y = f(x^{(i)}) = \dsum^d_{j=0}w_j (x^{(i)})^j = (\tilde x ^{(i)})^T w$ rewrite as:
\begin{align*}
  w^* &= \min_w \dsum^n_{i=1}\frac{1}{2}(y^{(i)} - \hat y^{(i)})^2\\
  &= \min_w \dsum^n_{i=1}\frac{1}{2}(y^{(i)} - (\tilde x ^{(i)})^T w)^2 \text{\qquad\qquad (LSR)}
\end{align*}
\end{samepage}

Solve $\norm{Xw-y}^2$ with Basis functions:
\[
X = \vct{f_1(x^{(1)}) & f_2(x^{(1)}) & \ldots & f_m(x^{(1)})\\
f_1(x^{(2)}) & f_2(x^{(2)}) & \ldots & f_m(x^{(2)})\\
\vdots & \vdots & \ddots & \vdots\\
f_1(x^{(n)}) & f_2(x^{(n)}) & \ldots & f_m(x^{(n)})} \qquad y = \vct{y^{(1)}\\y^{(2)}\\\vdots\\y^{(n)}}
\]

\subsection{Underfitting / Overfitting}

\bx{Underfitting}: Model too simple, degree low

\bx{Overfitting}: Model too complex, degree high

Too high model complexity $\to$ Higher training error

Lower polynomial degree or basis functions $\to$ Lower model complexity

\subsubsection{k-fold Cross Validation}

Mitigate Overfitting: Split training data into $k$ (usually $10$) and pick one for \bx{validation data}.

Train model on one training block, run on validation data and compute error. Repeat for all blocks and average.

\subsubsection{Regularization}

Constrain magnitude ($\norm{w}_2, \norm{w}_1$, etc.)

Lagrangian to remove constraint

\[
\begin{array}{ll}
  \min_w & L(w) \\
  st & \norm{w}_2^2 \leq t
\end{array} \qquad \to \qquad \min_w \quad L(w) + \frac{\lambda}{2} \norm{w}_2^2
\]

if $L(w) = \frac{1}{n} \dsumni l(y^{(i)}, \hat y^{(i)})$:
\be
\i Empirical risk minimization (ERM): \qquad $\min_w L(w)$
\i Regularized risk minimization (RRM): \quad $\min_w L(w) + \norm{w}$
\ee

\subsubsection{Bias-Variance Tradeoff}

Prediction error is sum of variance and bias

\bi
\i Variance spreads predictions around true value
\i Bias puts predictions away from true value
\ei

With complexer model:
\be
\i Test data has min somewhere
\i Bias gets lower
\i Variance gets higher
\ee

\subsubsection{Regularizers}

Ridge Regression: LSR with $\norm{w}_2$-regularizer:
\[
\min_w \frac{1}{2n} \norm{Xw-y}_2^2 + \frac{\lambda}{2} \norm{w}_2^2
\]

Least absolute shrinkage and selection operator (LASSO): $\norm{w}_1$-regularizer:
\[
\min_w \frac{1}{2n} \norm{Xw-y}_2^2 + \lambda \norm{w}_1
\]
Solved with subgrad method, performs feature selection.

Elastic Net: Combination of both
\[
\min_w \frac{1}{2n} \norm{Xw-y}^2_2 + \lambda \left( \alpha \, \norm{w}_1 + \frac{1-\alpha}{2}\, \norm{w}_2^2\right)
\]
Often used for gene expression data.

Robust Regression with $\norm{w}_1$-regularizer:
\[
\min_w \frac{1}{n} \norm{Xw-y}_1
\]
Solved with subgrad method. Often used with Huber Loss for faster, simpler optimization.

\subsection{Feature Scaling}

\bi
\i Features should be $[0,1]$ or $[-1,1]$
\i Regularizer not invariant to scaling
\i also on test data!
\ei

\newcommand{\centered}{\text{centered}}
\newcommand{\scaled}{\text{scaled}}
Normalize data: Center and scale each feature of data matrix $X_{i,j} = (x_j^{(i)})$
\[
X_{:,j}^{\centered} = X_{:,j} - \overline x_j = X_{:,j} - \frac{1}{n} \dsumni x_j^{(i)}
\]
\[
X_{:,j}^{\scaled} = \frac{X_{:,j}^{\centered}}{\norm{X_{:,j}^{\centered}}_2}
\]

\subsubsection{MLE and MAP}

Example: For Coin-throw with $p(\text{head}}) = \theta$: 3 heads, 7 tails.
What is most likely $\theta$? \[
p(y^{(1)}, y^{(2)}, \ldots, y^{(n)} \mid \theta ) = \dprod_i p(y^{(i)} \mid \theta) = \theta^3 (1-\theta)^7
\]

Maximum Likelihood Estimator (MLE): Find $\theta$ for max probability: \[
\max_{\theta} \theta^3 (1-\theta)^7
\]

\newcommand{\observation}{\text{observation}}
Maximum A Posteriori (MAP): Find $\theta$ for max probability with prior: \[
\max_{\theta} \theta^3 (1-\theta)^7 \cdot p(\theta \mid \observation)
\]
with $p(\theta \mid \observation ) = \frac{p(\observation \mid \theta) \cdot p(\theta)}{p(\observation)}$

Here, we maximize the product of the likelihood times the prior:
\[
\argmax_w p(w \mid X,y) = \argmax_w p(y \mid X,w) \cdot p(w)
\]
Which is the same as minimising the loss and the regularizer:
\[
\argmin_w \dsumni (y^{(i)} - (x^{(i)})^T w)^2}+ \lambda \norm{w}_2^2
\]
prior (variance) is regularizer.

\newline\newline
% create table ERM vs MLE
\begin{center}
\begin{tabular}{l|l}
Empirical risk min. & Maximum likelihood \\
\hline
Minimize & Maximize \\
Sum & Product \\
Risk / Loss function & Noise Distribution \\
$l_1$-loss & Gaussian Distribution\\
$l_2$-loss & Laplacian Distribution
\end{tabular}
\end{center}
Logarithmic equivalence

\subsection{Binary Classification}

\bi
\i $X = \RS^d$
\i $Y = \{-1,1\}$
\i training data $(x^{(i)}, y^{(i)})_{i=1..n}$
\i learn $f: X \to Y$
\i linear function (hyperplane in multidimensional space) $f(x) = x^Tw + b = 0$ returning $\sign(f(x))$
\ei

Loss functions:
\be
\i Logistic function
\bi 
\i penalizes points on correct side
\i close to decision boundary
\i asymptotical linear growth
\i MLE applied to logistic function $\log (1 + \exp(-t))$ is logistic regression
\i Logistic regression: $f(x) = \log(1 + \exp(-y \cdot (x^Tw + b)))$ 
\ei
\i Hinge loss: $f(x) = \max(0, 1 - t)$
\i Squared hinge loss: $f(x) = \max(0, 1 - t)^2$
\ee

\subsubsection{Support Vector Machine (SVM)}

Pick hyperplane with largest margin between classes.

\be
\i Hard margin SVM: No misclassified data points \[
\begin{array}{ll}
\min_{w \in \RS^d,b \in \RS} & \frac{1}{2} \norm{w}_2^2 \\
st & y^{(i)} \cdot (x^{(i)T}w + b) \geq 1
\end{array}
\]
\i Soft margin SVM: Allow errors \[
\begin{array}{ll}
\min_{w \in \RS^d,b \in \RS, \xi \in \RS^n} & \frac{1}{2} \norm{w}_2^2 + \frac{C}{n} \dsumni \xi_i \\
st & y^{(i)} \cdot ((x^{(i)}^T)w + b) \geq 1 - \xi_i \qquad , \xi_i \geq 0
\end{array}
\]
This would be with e.g. the hinge loss (minimize regularizer + empirical risk $\to$ RRM):
\[
\min_{w \in \RS^d,b \in \RS} \frac{1}{2} \norm{w}_2^2 + \frac{C}{n} \dsumni \max(0, 1 - y^{(i)} \cdot (x^{(i)T}w + b))
\]
\ee

Duality: If Primal problem convex and some conditions satisfied, optimal solution is equal to dual problem's solution (Strong duality).

In dual problems we only need scalar products of data points.

\be
\setcounter{enumi}{2}
\i Dual SVM: \[
\begin{array}{ll}
\max_{\alpha \in \RS^n} & \dsumni \alpha_i - \dsumni \dsumnj \alpha_i \alpha_j y^{(i)} y^{(j)} (x^{(i)})^T x^{(j)} \\
st & \dsumni \alpha_i y^{(i)} = 0 \\
& 0 \leq \alpha_i \leq \frac{C}{n}
\end{array}
\]
Used when many dimensions and hyperplane separator not linear and kernels are used.
\ee

\subsubsection{Kernels}

\bi
\i $x \in \RS^d$
\i map to $R^m$ using non-linear feature map $\phi$
\i Use linear SVM in $R^m$
\i never compute $\phi(x)$, only scalar products
\i use kernel function $k(x^{(i)}, x^{(j)})$
\ei


$k(a,b)$ is a kernel function... 
\bi 
\i If $\phi$ exists for $k(a,b) = \phi(a)\phi(b)$ for $a,b \in \RS$
\i If $\phi$ exists for $K = \phi(X)\phi(X)^T$ with data matrix $X$
\i Iff K is positive semidefinite ($K \succcurlyeq 0$) for any $n$ input points 
\bi
\i Sometimes $\phi$ maps to $\RS^\infty$ (Reproducing Kernel Hilbert Space)
\ei
\ei

Examples
\bi
\i Linear kernel for $k: \RS^d \times \RS^d \to \RS$ \[k(x^{(i)}, x^{(j)}) = (x^{(i)})^T x^{(j)}\]
\i Cosine kernel: Assume data have all norm 1: $\norm{x^{(i)}}_2 = 1$, angle measures similarity
\[
\cos(x^{(i)}, x^{(j)}) = \frac{(x^{(i)})^T x^{(j)}}{\norm{x^{(i)}} \norm{x^{(j)}}} = (x^{(i)})^T x^{(j)}
\]
\i Gaussian kernel (Radial basis function) \[
k(x^{(i)}, x^{(j)}) = \exp\left(-\frac{\norm{x^{(i)} - x^{(j)}}_2^2}{2\sigma^2}\right)
\]
maps to infinite dimensions
\i Polynomial Kernel for $c > 0$ and $s \in \NS$:
\[
k(x^{(i)}, x^{(j)}) = (x^{(i)})^T x^{(j)} + c)^s
\]
\i Sum and positive scalar product of kernels are kernels
\ei
\newpage
Kernel for Regularized Risk Minimization if norm from 2:
\bi
\i $\min_w L(w,X,y) + \lambda R(w)$
\i iff $R = h(\norm{w}_2)$ for non-decreasing $h$, then $w^* = \dsum_i \beta_i \phi(x^{(i)})$
\i LSR: $\min_w \norm{Xw-y}^2_2 \Rightarrow \min_\beta \norm{K\beta - y}^2_2$ 
\i Ridge Regression: $\min_w \norm{Xw-y}^2_2 + \lambda \norm{w}_2^2 \Rightarrow \min_\beta \norm{K\beta - y}^2_2 + \lambda \beta^T K \beta$
\i Regularized Logistic Regression, etc.
\ei

\subsection{Parametric vs Nonparametric Models}

\bi
\i Parametric if model depends on fixed number of parameters
\i Nonparametric e.g. SVM with Gaussian kernel
\ei

\subsection{Multiclass Classification}

\bi
\i One-vs-Rest: Train binary classifier for each class-pair, predict highest score class
\i One-vs-One: Train binary classifier for each class-pair, predict class with most votes
\i Direct methods: e.g. K-nearest neighbor 
\ei 

\subsubsection{K-nearest neighbor}

\bi
\i Find $k$ nearest neighbors in training data
\i Predict class with most votes
\i Choose $k$ with cross-validation
\i Small k $\to$ overfitting, large k $\to$ underfitting
\i Advantage: Easy, simple, no training phase, non-parametric
\i Disadvantage: Computationally expensive, sensitive to Noise
\ei

Regression: Compute average of $k$ nearest neighbors \[y = \frac{1}{k} \dsum_{i \in S} y^{(i)}\] or weighted \[y = \frac{1}{k} \dsum_{i \in S} \frac{1}{\norm{x-x^{(i)}}} \cdot y^{(i)}\]

\subsubsection{Naive Bayes}

\bi
\i Probabilistic ML algorithm
\i Assume features are conditionally independent
\i Compute $p(y \mid x) = \frac{p(x \mid y) \cdot p(y)}{p(x)}$
\i $p(y \mid x)$ is event $y$ in class $x$
\i Bayesian theorem: $p(y \mid x) = \frac{p(x \mid y) \cdot p(y)}{p(x)}$
\i Laplacian Smoothing for zero values: e.g. with words $w_i$ of class $x$ \[
p(w_i \mid x) = \frac{\text{frequency of $w_i$} +1 }{\text{words in $x$} + \text{total unique words}}
\]
\ei

\subsubsection{Decision Trees}

\bi
\i $X = \RS^d$
\i $Y = {1,2,...,m}$
\i Recursive partitioning of data
\i Split data into subsets
\i Choose feature and threshold to split (entropy, gini index)
\i Repeat until stopping criterion
\i Predict class with majority vote
\i Advantage:
\i Disadvantage: Overfitting, sensitive to noise
\ei

\subsubsection{Ensemble Learning}

\bi
\i Combine multiple models to improve performance
\i Bagging: Train multiple models on random subsets of data, smaller variance
\i Bootstrapping: Randomly sample data with replacement
\i Boosting: Train multiple models sequentially, each model corrects previous model
\ei

\subsubsubsection{Random Forest}

\bi
\i Bagging with decision trees
\i Each decision tree trained on random bootstrap subset of size $m$ of data of size $n$
\i Construct decision tree $T_b$:
\bi
\i For each node that contains more than $n_{min}$ data points:
\i Randomly select $p$ of $d$ features
\i Split node with best feature and threshold
\i Repeat until all nodes smaller
\ei
\i Output: Random decision trees $T_1, T_2, \ldots, T_B$
\ei

\section{Generative Models}

\bi
\i Discrimative: Learn $p(y \mid x)$
\i Generative: Learn $p(y \mid x) \cdot p(x) = p(x, y)$ joint probability
\ei

\subsection{Discriminant Analysis}

\subsection{Gaussian Discriminant Analysis}

\bi
\i generate new data, also for classification
\i $\mu = \frac{1}{n} \dsum_i x^{(i)}$
\i $\Sigma = \frac{1}{n} \dsum_i (x^{(i)} - \mu) (x^{(i)} - \mu)^T$
\i for each class, get $\mu_c$ and $\Sigma_c$
\i estimate $p(x,y) = p(x \mid y) \cdot p(y)$ with $p(y=c) = \frac{n_c}{n}$ where $n_c$ is number of data points in class $c$ and $N$ is total amount of data points
\i this becomes $x^T \Sigma x + v^Tx + t=0$
\i Boundary is quadratic
\i number of parameters to be estimated: $2 \frac{d(d+1)}{2} + 2d$
\ei

\subsection{Linear Discriminant Analysis}

\bi
\i LSR with $y \in \{-1, +1\}\}$: $\min_w \norm{Xw-y}_2^2$
\i numbers to be estimated: $d+1$
\i For generation:
\bi
\i Assume $\Sigma = \Sigma_{-1} = \Sigma_{+1}$ for all classes
\i number of parameters to be estimated: $\frac{d(d+1)}{2} + 2d$ (much higher than for discriminative models)
\ei
\ei

\section{Supervised learning}

\bi
\i predict label for $(x^{(i)})_i$
\i find structure, possibly assign labels
\i can generate new data
\ei

\subsection{k-means Objective}

\bi
\i for $x^{(i)} \in \RS^d$ assume given data points $(x^{(i)})^n_{i=1}$
\i group into clusters $C = \{C_1, C_2, \ldots, C_k\}$
\i construct class $k$ class centers $\mu_i$ representing groups
\i Find centers such that sum of squared distances of data points to closest center is minimized: \[
\min_{\mu_1, \ldots, \mu_k \in \RS^d, C} \dsum_{j=1}^k \dsum_{x^{(i)} \in C_j} \norm{x^{(i)} - \mu_j}^2
\]
\i non-convex min. problem
\ei

\subsubsection{LLoyds algorithm}

\be
\i Input: data points $x^{(1)}, \ldots, x^{(n)} \in \RS^d$, number $k$ of clusters
\i randomly init centers $\mu^{(0)}_1,\ldots,\mu^{(0)}_k$
\i while not converged:
\be
\i assign each data point to closest center $C^{(i+1)}_1, \ldots, C^{(i+1)}_k$ with \[
x^{(s)} \in C^{(i+1)}_j \Leftrightarrow \norm{x^{(s)} - \mu_j^{(i)}}^2 \leq \norm{x^{(s) - \mu_l^{i}}}^2, l = 1, \ldots, k
\] ($x^{(s)}$ is assigned to cluster $C^{(i+1)}_j$ if it is closest to $\mu_j$)
\i compute new cluster centers \[
\mu_j^{(i+1)} = \frac{1}{\vert C^{(i+1)}_j \vert} \dsum_{x^{(s)} \in C^{(i+1)}_j} x^{(s)}
\]
\ee
\i Output: $C_1, \ldots, C_k$
\ee

This algorithm can be stuck in local optimum. Finding global optimum is NP-hard.

\subsubsection{Alternatives}

\bi
\i k-median. $\norm{\cdot}_2^2$ instead of $\norm{\cdot}_2$
\bi
\i more robust to outliers
\ei
\i weighted k-means: weights for individual data-points
\i soft k-means: no hard assignments, but probabilities
\ei

\subsection{Principal Component Analysis (PCA)}

\bi
\i Dimensionality reduction technique
\i identify patterns and relationsships in high-D data
\i many approaches, e.g.:
\bi
\i Maximize variance of projected data $\to$ covariance matrix
\i Minimize squared error $\to$ SVD
\ei
\ei

\subsubsection{Maximize Variance}

\be
\i Input: Data points $x^{(1)}, \ldots, x^{(n)} \in \RS^d$, parameter $l < d$ for reduced dimension
\i Output: Projection $\pi_S$ on affine subspace $S$ so variance of projected points is maximized: \[
\max_S \Var_l(\pi_S(X))
\]
\i Assume data is centered: $\bar x = \frac{1}{n} \dsumni x^{(i)} = 0$ or center by $\tilde x^{(i)} = x^{(i)} - \bar x$
\i Compute Covariance Matrix:
\bi
\i For one-dimensional case $l=1$:
\begin{align*}
&
\begin{array}{cl}
\max_{a \in \RS^d} & \Var(\pi_a(X)) \\
\st & \norm{a} = 1
\end{array} \\
\Rightarrow&
\begin{array}{cl}
\max_{a \in \RS^d} & \sum_{i=1}^n (\pi_a(x^{(i)}))^2\\
\st & \norm{a} = 1
\end{array} \\
\Rightarrow&
\begin{array}{cl}
\max_{a \in \RS^d} & \sum_{i=1}^n (a^Tx^{(i)})^2\\
\st & \norm{a} = 1
\end{array} \\
\Rightarrow&
\begin{array}{cl}
\max_{a \in \RS^d} & \norm{Xa}^2\\
\st & \norm{a} = 1
\end{array} \\
\Rightarrow&
\begin{array}{cl}
\max_{a \in \RS^d} & a^T(X^TX)a\\
\st & \norm{a}^2 = 1
\end{array} \\
\end{align*}
\i Solution is eigenvector $v$ to largest eigenvalue $\lambda_{max}$ of covariance matrix $X^TX$
\i for bigger projected dimensions $l > 1$ project data on space spanned by eigenvectors of the $l$ largest eigenvalues of $C = X^TX$
\ei
\i Compute Eigendecomposition $C = V \Lambda V^T$
\i Define $V_l$ as the matrix containing the $l$ largest eigenvectors, i.e. first $l$ columns of $V$ if eigenvalzes in $\Lambda$ are decreasing order
\i Compute new data points 
\bi
\i $y^{(i)} = V_l^T \tilde x^{(i)} \in \RS^l$ is the projection of $x^{(i)}$ on $l$-dimensional subspace
\bi
\i distance between point and projection is reconstruction error
\i Reconstruction error bound by $\dsum_{k=l+1}^d \lambda_k$
\ei
\i $z^{(i)} = P \tilde x^{(i)} + \bar x \in \RS^d$ with $P = V_l V_l^T$ to map points back to original $d$-dimensional space (they only span $l$-dimensional affine space now)
\ei
\ee

\subsubsection{Kernel PCA}

\bi
\i With $x^{(i)},\ldots,x^{(n)}$ stacked in data matrix X as rows
\i Covariance matrix is $C=X^TX \in \RS^{d\times d}$ with $d$ eigenvalues
\i Kernel matrix is $K = XX^T \in \RS^{n\times n}$ with $n$ eigenvalues
\i Express eigenvalues/eigenvectors of C by those of K and vice versa
\ei

\subsection{Low-dimensional Embedding}

\bi
\i Given distance matrix $D \in \RS^{n \times n}$ containing $d_{ij} = \norm{x_i-x_j}$ between data points
\i Recover $(x^{(i)})_{i=1,\ldots,n} \in \RS^d$
\i find $\phi: X \to \RS^d$ for $\norm{\phi(x^{(i)}) - \phi(x^{(j)})} = d_{ij}$
\i if $D$ from points in $\RS^d$, $D$ is Euclidean distance matrix (perfect $\phi$ computable)
\ei

\subsubsection{Multidimensional Scaling (MDS)}

\bi
\i Given distance matrix $D$, find $x^{(i)}$ that best-match $D$
\i Classic MDS: Minimize error wrt pairwise scalar product
\i Metric MDS: Minimize error wrt distance matrix $D$
\i Non-metric MDS: $D$ is not a distance matrix but has ordering $\to$ preserve ordering
\i non-convex, NP-hard
\ei

\subsubsubsection{Classic MDS}
\be
\i The problem is: \begin{align*}
&\min_{x^{(1)}, \ldots, x^{(n)}} \dsum_{i,j} (d_{i,j} - (x^{(i)})^Tx^{(j)})^2 \\
\Rightarrow & \min_{X \in \RS^{n \times d}} \norm{S-XX^T}^2_{Fro} 
\end{align*}
\i Compute matrix $S = \frac{1}{2} (d^2_{1,i} + d^2_{1,j} + d^2_{i,j})$
\i Compute eigenvalue decomposition $S = V \Lambda V^T$
\i $V_d$ is matrix containing $d$ largest eigenvectors of $S$
\i $\Lambda_d$ is is $d \times d$ diagonal matrix with first $d$ eigenvalues on diagonal
\i $X = V_d \sqrt{\Lambda_d}$
\i row $i$ of $X$ is coordinates of embedded point $x^{(i)}$
\ee

Note: If data points are in $\RS^d$, $S$ has rank $d$, and $d$ eigenvalues $>0$ and $n-d$ eigenvalues $=0$. Spectrum of $S$ provides information on $d$.

\subsubsubsection{(Non-)Metric MDS}

\bi
\i Metric MDS: \[\min_{x^{(1)}, \ldots, x^{(n)}} \dsum_{i,j} ( \norm{x^{(i)} - x^{(j)}} - d_{i,j})^2 \]
\i Non-Metric MDS: Distance matrix consists of $d_{i,j} > d_{i,k}$: \[\min_{x^{(1)}, \ldots, x^{(n)}} \dsum_{i,j} ( f(\norm{x^{(i)} - x^{(j)}}) - d_{i,j})^2\]
\ei

\subsection{Isomap}

\bi
\i Detect lower-dimensional structures in data
\i Keep local distances of points
\i Compute geodesic distances for large distances with k-nearest neighbor graph, shortest path distances approximate geodesic distances
\i Store distances in distance matrix $D$
\i Metric MDS on $D$ (preserves geodesic distances)
\ei

\subsection{Recommender Systems}

\bi
\i Fill out missing values in data matrix
\i For $U,V$, we can compute whole rating matrix $M = U \cdot V^T$
\i Sparse matrix $R$ that matches predictions
\i Goal: Find $U, V$ that predicition matches input ratings \begin{align*}
& \min_{U \in \RS^{m \times k}, V \in \RS^{n \times k}} \dsum_{(i,j) \in \Omega} (r_{i,j} - (u^{(i)})^T v^{(j)})^2 \\
= & \min_{U \in \RS^{m \times k}, V \in \RS^{n \times k}} \norm{R - UV^T}^2_{\Omega} \qquad \text{where } \qquad \norm{A}_\Omega^2 = \dsum_{(i,j) \in \Omega} a_{i,j}^2\\
= & \min_{U \in \RS^{m \times k}, V \in \RS^{n \times k}} \norm{R - UV^T}^2_{Fro}
\end{align*}
\i Non-convex, NP-hard
\i Gradient descent or alternating least squares (fix $U$ or $V$ and optimize the other, then switch)
\ei

\subsection{Topic Modelling}

\bi
\i Word frequency transformation
\i rare and frequent words discarded
\i $w_{i,j}$ means how often word $j$ appears in document $i$
\i Term Frequency $\text{TF} = \frac{w_{i,j}}{\text{length of document i}}$
\i Inverse Document Frequency $\text{IDF} = \log \frac{N}{\text{number of documents containing word j}}$
\i smoothed IDF: $\text{IDF} = \log \frac{N+1}{\text{number of documents containing word j} + 1} + 1$
\i TF-IDF: $\text{TF-IDF} = \text{TF} \cdot \text{IDF}$
\i in Matrix $W$ store TF-IDF values
\i Kullback-Leibler divergence computation on $W$ to find topics
\i Gets feature vectors for words, can assign topics to words and documents
\ei

\subsection{Matrix Factorization}

\bi
\i Useful for images
\i Uncovers latent factors (topics)
\i Imputes data
\ei