\npsection{Introduction}

ML: Tries to automate the process of \bx{inductive inference}.

\be
\i Deduction: Learning from rules
\i Induction: Learning from examples
\ee

\subsection{Math}

\TODO{norms}
\TODO{determinant, trace, inverse}
\TODO{semidefinite, definite, indefinite}
\TODO{linear eq}
\TODO{inverse proof}

\subsubsection{Eigenvalues and Eigenvectors}

Example: $f(w) = 0.5 w^TMw$
\bi
\i Hessian: $\nabla^2 f(w) = M = \vct{1 & 0 \\ 0 & 3}$
\i Eigenvalues $1,3$
\i Function along eigenvectors like $1x^2$ and $3x^2$
\ei

\subsubsection{Derivative and Hessian}

\newcommand{\dlw}[2]{\frac{\partial L_#1}{\partial w_#2}}
\begin{align*}
L(w): \RS \to \RS^m
\Rightarrow  \nabla L(w) = \vct{\drv{w_1}L(w)\\\vdots\\\drv{w_n}L(w)}
\Rightarrow \nabla L(w) = \left(\begin{array}{cccc}
\dlw{1}{1} & \dlw{2}{1} & \ldots & \dlw{m}{1} \\
\dlw{1}{2} & \dlw{2}{2} & \ldots & \dlw{m}{2} \\
\vdots & \vdots & \ddots & \vdots \\
\dlw{1}{d} & \dlw{2}{d} & \ldots & \dlw{m}{d}
\end{array}\right)
\end{align*}

\newcommand{\dfdx}[2]{
    \frac{\partial^2 f}{\partial x_{#1} \partial x_{#2}} (x)
}
\begin{align*}
\nabla^2 f(x) =
\vct{
    \dfdx{1}{1} & \ldots & \dfdx{1}{d} \\
    \vdots & \ddots & \vdots \\
    \dfdx{d}{1} & \ldots & \dfdx{d}{d} 
}, \qquad x = \vct{x_1\\\vdots\\x_d} \in \RS^d
\end{align*}

\npsection{Supervised learning}

\bi
\i input $X$, output $Y$
\i training data: $(x^{(i)}, y^{(i)})_{i=1..n} \subset X \times Y$
\i Goal: learn $f: X \to Y$ for model class $F$ on examples
\ei

\subsection{Least squares regression}
$\tilde X, \tilde w$ are extended with bias:
\begin{align*}
\min_{\tilde w} \frac{1}{2} \norm{\tilde X \tilde w - y}^2 \Rightarrow \min_w \frac{1}{2} \norm{Xw-y}^2
\end{align*}
Solve with gradient and set to zero:
\begin{align*}
L &= \frac{1}{2} \dsum_{i=1}^n ((X_i^Tw_i)-y_i)^2 \\
&= \frac{1}{2} \left(\dsum_{i=1}^n (X_i^Tw_i)^2 - 2(X_i^Tw_i)y_i + y_i^2 \right) \\
\end{align*}
\begin{align*}
  \nabla L &= \drv{w} \left(  \frac{1}{2} \left(\dsum_{i=1}^n (X_i^Tw_i)^2 - 2(X_i^Tw_i)y_i + y_i^2 \right) \right)\\
  &= \frac{1}{2} \left(\dsum_{i=1}^n 2(X_i^TX_iw_i) - 2(X_i^T)y_i \right)\\
  &= \dsum_{i=1}^n X_i^TX_iw_i - X_i^Ty_i\\
  &= X^TXw - X^Ty\\
  &= X^T(Xw-y)
\end{align*}
\begin{align*}
\nabla L = X^T(Xw-y) = 0 \Rightarrow (X^TX)w = X^Ty \Rightarrow w = (X^TX)^{-1}X^Ty
\end{align*}

\subsection{Gradient descent}
Alternative to least squares regression. Algorithm:
\be
\i Compute gradient $\nabla L(w) = X^T(Xw-y)$
\i Negative gradient shows to steepest descent
\i $w^{(t+1)} = w^{(t)} - \gamma^{(t)} \cdot \nabla L(w^{(t)})$
\ee

\subsection{Derivative examples}

\bi
\i $L(w) = w_1^2 + w_2^2 \\\Rightarrow \nabla L(w) = \vct{2w_1\\2w_2}$
\i $L(w) = \norm{w} ^2_2 = w^Tw \\\Rightarrow \nabla L(w) = 2w$
\i $L(w) = w^TAw \\\Rightarrow \nabla L(w) = Aw + A^Tw$
\i $L(w) = \norm{Xw-y}^2 = w^TX^TXw - y^TXw - w^TX^Ty + y^Ty \\\Rightarrow \nabla L(w) = 2X^T(Xw - y)$
\ei

\subsection{Convexity}

Set $C$ convex if line between any two points of $C$ in $C$. $\forall x,y \in C$ and $\lambda \in \RS$ with $0 \leq \lambda \leq 1$: \[\lambda x + (1 - \lambda) y \in C\]

Function $f: \RS^d \to \RS$ convex if $\dom(f)$ is a convex set and $\forall x,y \in \dom(f)$, $\lambda \in \RS$ with $0 \leq \lambda \leq 1$: \[f(\lambda x + (1 - \lambda) y) \leq \lambda f(x) + (1 - \lambda) f(y)\]

Gradient descent returns global optimum for convex functions.

Optimization problem: $\min f(x), x \in X \subseteq \RS^d$ has local minimizer $x^* \in X$ if $\exists \epsilon > 0$ with:
\[\forall y \in X \text{ with } \norm{x^* - y} \leq \epsilon: f(x^*) \leq f(y)\]
Global minimizer if $f(x^*)$ is lowest of all optimizers.

Symmetric matrix $A$ is positive semidefinite ($A \succcurlyeq 0$) if :
\[x^TAx \geq 0, \forall x\]
Positive definite ($A \succ 0$) if $\forall x \neq 0$

Symmetric matrix $A$ is positive semidefinite iff all eigenvalues are $\geq 0$ and positive definite iff all $> 0$.

If function is one-dimensional: Convex if $f''(x) \geq 0$.
If multidimensional: Convex if 2nd derivative is psd.


\subsection{Backtracking line search} % not needed in exam

Algorithm:
\be
\i Input: $x, \Delta x, \alpha \in (0,0.5), \beta \in (0,1)$
\i $t = 1$
\i while $f(x + t \, \Delta x) > f(x) + \alpha \, t \, \nabla f(x)^T \, \Delta x$:
\i \quad $t = \beta \, t$
\ee

\subsection{Solve LSR}

\be
\i $L(w) = \frac{1}{2} \norm{Xw-y}_2^2$
\i $\nabla L(w) = X^T(Xw-y)$
\i $\nabla L(w) = X^TX$ is symmetric and psd
\ee

\subsection{Subgradient method} % not needed in exam

If function not differentiable, e.g. $\norm{w}_1$
\bi
\i gradient is subgradient (convex hull of gradients)
\i choose constant step length
\ei