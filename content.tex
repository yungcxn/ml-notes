\npsection{Introduction}

ML: Tries to automate the process of \bx{inductive inference}.

\be
\i Deduction: Learning from rules
\i Induction: Learning from examples
\ee

\subsection{Math}

\TODO{norms}
\TODO{determinant, trace, inverse}
\TODO{semidefinite, definite, indefinite}
\TODO{linear eq}
\TODO{inverse proof}

\subsubsection{Eigenvalues and Eigenvectors}

Example: $f(w) = 0.5 w^TMw$
\bi
\i Hessian: $\nabla^2 f(w) = M = \vct{1 & 0 \\ 0 & 3}$
\i Eigenvalues $1,3$
\i Function along eigenvectors like $1x^2$ and $3x^2$
\ei

\subsubsection{Derivative and Hessian}

\newcommand{\dlw}[2]{\frac{\partial L_#1}{\partial w_#2}}
\begin{align*}
L(w): \RS \to \RS^m
\Rightarrow  \nabla L(w) = \vct{\drv{w_1}L(w)\\\drv{w_2}L(w)\\\vdots\\\drv{w_n}L(w)}
\Rightarrow \nabla L(w) = \left(\begin{array}{cccc}
\dlw{1}{1} & \dlw{2}{1} & \ldots & \dlw{m}{1} \\
\dlw{1}{2} & \dlw{2}{2} & \ldots & \dlw{m}{2} \\
\vdots & \vdots & \ddots & \vdots \\
\dlw{1}{d} & \dlw{2}{d} & \ldots & \dlw{m}{d}
\end{array}\right)
\end{align*}

\newcommand{\dfdx}[2]{
    \frac{\partial^2 f}{\partial x_{#1} \partial x_{#2}} (x)
}
\begin{align*}
\nabla^2 f(x) =
\vct{
    \dfdx{1}{1} & \ldots & \dfdx{1}{d} \\
    \vdots & \ddots & \vdots \\
    \dfdx{d}{1} & \ldots & \dfdx{d}{d} 
}, \qquad x = \vct{x_1\\\vdots\\x_d} \in \RS^d
\end{align*}

\subsubsection{Bonus}

Convex hull $\conv(V)$ for set of vectors $V$ is smallest convex set containing $V$.
\[ \conv(V) = \left\{\dsum^m_{i=1} \lambda_i \cdot v_i \mid \lambda_i \geq 0, \dsum^m_{i=1} \lambda_i = 1\right\} \]

\npsection{Supervised learning}

\bi
\i input $X$, output $Y$
\i training data: $(x^{(i)}, y^{(i)})_{i=1..n} \subset X \times Y$
\i Goal: learn $f: X \to Y$ for model class $F$ on examples
\ei

\subsection{Least squares regression}
$\tilde X, \tilde w$ are extended with bias:
\begin{align*}
\min_{\tilde w} \frac{1}{2} \norm{\tilde X \tilde w - y}^2 \Rightarrow \min_w \frac{1}{2} \norm{Xw-y}^2
\end{align*}
Solve with gradient and set to zero:
\begin{align*}
L &= \frac{1}{2} \dsum_{i=1}^n ((X_i^Tw_i)-y_i)^2 \\
&= \frac{1}{2} \left(\dsum_{i=1}^n (X_i^Tw_i)^2 - 2(X_i^Tw_i)y_i + y_i^2 \right) \\
\end{align*}
\begin{align*}
  \nabla L &= \drv{w} \left(  \frac{1}{2} \left(\dsum_{i=1}^n (X_i^Tw_i)^2 - 2(X_i^Tw_i)y_i + y_i^2 \right) \right)\\
  &= \frac{1}{2} \left(\dsum_{i=1}^n 2(X_i^TX_iw_i) - 2(X_i^T)y_i \right)\\
  &= \dsum_{i=1}^n X_i^TX_iw_i - X_i^Ty_i\\
  &= X^TXw - X^Ty\\
  &= X^T(Xw-y)
\end{align*}
\begin{align*}
\nabla L = X^T(Xw-y) = 0 \Rightarrow (X^TX)w = X^Ty \Rightarrow w = (X^TX)^{-1}X^Ty
\end{align*}

\subsection{Gradient descent}
Alternative to least squares regression. Algorithm:
\be
\i Compute gradient $\nabla L(w) = X^T(Xw-y)$
\i Negative gradient shows to steepest descent
\i $w^{(t+1)} = w^{(t)} - \gamma^{(t)} \cdot \nabla L(w^{(t)})$
\ee

\subsubsection{Derivative examples}

\bi
\i $L(w) = w_1^2 + w_2^2 \\\Rightarrow \nabla L(w) = \vct{2w_1\\2w_2}$
\i $L(w) = \norm{w} ^2_2 = w^Tw \\\Rightarrow \nabla L(w) = 2w$
\i $L(w) = w^TAw \\\Rightarrow \nabla L(w) = Aw + A^Tw$
\i $L(w) = \norm{Xw-y}^2 = w^TX^TXw - y^TXw - w^TX^Ty + y^Ty \\\Rightarrow \nabla L(w) = 2X^T(Xw - y)$
\ei

\subsubsection{Convexity}

Set $C$ convex if line between any two points of $C$ in $C$. $\forall x,y \in C$ and $\lambda \in \RS$ with $0 \leq \lambda \leq 1$: \[\lambda x + (1 - \lambda) y \in C\]

Function $f: \RS^d \to \RS$ convex if $\dom(f)$ is a convex set and $\forall x,y \in \dom(f)$, $\lambda \in \RS$ with $0 \leq \lambda \leq 1$: \[f(\lambda x + (1 - \lambda) y) \leq \lambda f(x) + (1 - \lambda) f(y)\]

Gradient descent returns global optimum for convex functions.

Optimization problem: $\min f(x), x \in X \subseteq \RS^d$ has local minimizer $x^* \in X$ if $\exists \epsilon > 0$ with:
\[\forall y \in X \text{ with } \norm{x^* - y} \leq \epsilon: f(x^*) \leq f(y)\]
Global minimizer if $f(x^*)$ is lowest of all optimizers.

Symmetric matrix $A$ is positive semidefinite ($A \succcurlyeq 0$) if :
\[x^TAx \geq 0, \forall x\]
Positive definite ($A \succ 0$) if $\forall x \neq 0$

Symmetric matrix $A$ is positive semidefinite iff all eigenvalues are $\geq 0$ and positive definite iff all $> 0$.

If function is one-dimensional: Convex if $f''(x) \geq 0$.
If multidimensional: Convex if 2nd derivative is psd.


\subsubsection{Backtracking line search} % not needed in exam

Algorithm:
\be
\i Input: $x, \Delta x, \alpha \in (0,0.5), \beta \in (0,1)$
\i $t = 1$
\i while $f(x + t \, \Delta x) > f(x) + \alpha \, t \, \nabla f(x)^T \, \Delta x$:
\i \quad $t = \beta \, t$
\ee

\subsubsection{Solve LSR}

\be
\i $L(w) = \frac{1}{2} \norm{Xw-y}_2^2$
\i $\nabla L(w) = X^T(Xw-y)$
\i $\nabla L(w) = X^TX$ is symmetric and psd
\ee

\subsubsection{Subgradient method} % not needed in exam

If function not differentiable, e.g. $\norm{w}_1$
\bi
\i gradient is subgradient (convex hull of gradients)
\i choose constant step length $g$
\i $w^{(t+1)} = w^{(t)} - \gamma^{(t)} \cdot g$ with $\gamma^{(t)} = \frac{1}{\sqrt{t}}$
\i find $g \in \RS^d$ at $x \in \dom(f)$ with: \[f(y) \geq f(x) + g^T(y-x), \forall y \in \dom(f)\]
\ei

\subsection{Polynomial Regression}

\bi
\i $X \in \RS, Y \in \RS$
\i $f(x) = w_dx^d + w_{d-1}x^{d-1} + \ldots + w_1x^1 + w_0$
\i find best $w = (w_d, \ldots, w_0) \in \RS^{d+1}$
\i loss function is squared loss: $l(y, \hat y) = \frac{1}{2}(y - \hat y)^2$
\ei

\begin{samepage}
With $\hat y = f(x^{(i)}) = \dsum^d_{j=0}w_j (x^{(i)})^j = (\tilde x ^{(i)})^T w$ rewrite as:
\begin{align*}
  w^* &= \min_w \dsum^n_{i=1}\frac{1}{2}(y^{(i)} - \hat y^{(i)})^2\\
  &= \min_w \dsum^n_{i=1}\frac{1}{2}(y^{(i)} - (\tilde x ^{(i)})^T w)^2 \text{\qquad\qquad (LSR)}
\end{align*}
\end{samepage}

Solve $\norm{Xw-y}^2$ with Basis functions:
\[
X = \vct{f_1(x^{(1)}) & f_2(x^{(1)}) & \ldots & f_m(x^{(1)})\\
f_1(x^{(2)}) & f_2(x^{(2)}) & \ldots & f_m(x^{(2)})\\
\vdots & \vdots & \ddots & \vdots\\
f_1(x^{(n)}) & f_2(x^{(n)}) & \ldots & f_m(x^{(n)})} \qquad y = \vct{y^{(1)}\\y^{(2)}\\\vdots\\y^{(n)}}
\]

\subsection{Underfitting / Overfitting}

\bx{Underfitting}: Model too simple, degree low

\bx{Overfitting}: Model too complex, degree high

Too high model complexity $\to$ Higher training error

Lower polynomial degree or basis functions $\to$ Lower model complexity

\subsubsection{k-fold Cross Validation}

Mitigate Overfitting: Split training data into $k$ (usually $10$) and pick one for \bx{validation data}.

Train model on one training block, run on validation data and compute error. Repeat for all blocks and average.

\subsubsection{Regularization}

Constrain magnitude ($\norm{w}_2, \norm{w}_1$, etc.)

Lagrangian to remove constraint

\[
\begin{array}{ll}
  \min_w & L(w) \\
  st & \norm{w}_2^2 \leq t
\end{array} \qquad \to \qquad \min_w \quad L(w) + \frac{\lambda}{2} \norm{w}_2^2
\]

if $L(w) = \frac{1}{n} \dsumni l(y^{(i)}, \hat y^{(i)})$:
\be
\i Empirical risk minimization (ERM): \qquad $\min_w L(w)$
\i Regularized risk minimization (RRM): \quad $\min_w L(w) + \norm{w}$
\ee

\subsubsection{Bias-Variance Tradeoff}

Prediction error is sum of variance and bias

\bi
\i Variance spreads predictions around true value
\i Bias puts predictions away from true value
\ei

With complexer model:
\be
\i Test data has min somewhere
\i Bias gets lower
\i Variance gets higher
\ee

\subsubsection{Regularizers}

Ridge Regression: LSR with $\norm{w}_2$-regularizer:
\[
\min_w \frac{1}{2n} \norm{Xw-y}_2^2 + \frac{\lambda}{2} \norm{w}_2^2
\]

Least absolute shrinkage and selection operator (LASSO): $\norm{w}_1$-regularizer:
\[
\min_w \frac{1}{2n} \norm{Xw-y}_2^2 + \lambda \norm{w}_1
\]
Solved with subgrad method, performs feature selection.

Elastic Net: Combination of both
\[
\min_w \frac{1}{2n} \norm{Xw-y}^2_2 + \lambda \left( \alpha \, \norm{w}_1 + \frac{1-\alpha}{2}\, \norm{w}_2^2\right)
\]
Often used for gene expression data.

Robust Regression with $\norm{w}_1$-regularizer:
\[
\min_w \frac{1}{n} \norm{Xw-y}_1
\]
Solved with subgrad method. Often used with Huber Loss for faster, simpler optimization.

\subsection{Feature Scaling}

\bi
\i Features should be $[0,1]$ or $[-1,1]$
\i Regularizer not invariant to scaling
\i also on test data!
\ei

\newcommand{\centered}{\text{centered}}
\newcommand{\scaled}{\text{scaled}}
Normalize data: Center and scale each feature of data matrix $X_{i,j} = (x_j^{(i)})$
\[
X_{:,j}^{\centered} = X_{:,j} - \overline x_j = X_{:,j} - \frac{1}{n} \dsumni x_j^{(i)}
\]
\[
X_{:,j}^{\scaled} = \frac{X_{:,j}^{\centered}}{\norm{X_{:,j}^{\centered}}_2}
\]

\subsubsection{MLE and MAP}

Example: For Coin-throw with $p(\text{head}}) = \theta$: 3 heads, 7 tails.
What is most likely $\theta$? \[
p(y^{(1)}, y^{(2)}, \ldots, y^{(n)} \mid \theta ) = \dprod_i p(y^{(i)} \mid \theta) = \theta^3 (1-\theta)^7
\]

Maximum Likelihood Estimator (MLE): Find $\theta$ for max probability: \[
\max_{\theta} \theta^3 (1-\theta)^7
\]

\newcommand{\observation}{\text{observation}}
Maximum A Posteriori (MAP): Find $\theta$ for max probability with prior: \[
\max_{\theta} \theta^3 (1-\theta)^7 \cdot p(\theta \mid \observation)
\]
with $p(\theta \mid \observation ) = \frac{p(\observation \mid \theta) \cdot p(\theta)}{p(\observation)}$

Here, we maximize the product of the likelihood times the prior:
\[
\argmax_w p(w \mid X,y) = \argmax_w p(y \mid X,w) \cdot p(w)
\]
Which is the same as minimising the loss and the regularizer:
\[
\argmin_w \dsumni (y^{(i)} - (x^{(i)})^T w)^2}+ \lambda \norm{w}_2^2
\]
prior (variance) is regularizer.

\newline\newline
% create table ERM vs MLE
\begin{center}
\begin{tabular}{l|l}
Empirical risk min. & Maximum likelihood \\
\hline
Minimize & Maximize \\
Sum & Product \\
Risk / Loss function & Noise Distribution \\
$l_1$-loss & Gaussian Distribution\\
$l_2$-loss & Laplacian Distribution
\end{tabular}
\end{center}
Logarithmic equivalence

\subsection{Binary Classification}

\bi
\i $X = \RS^d$
\i $Y = \{-1,1\}$
\i training data $(x^{(i)}, y^{(i)})_{i=1..n}$
\i learn $f: X \to Y$
\i linear function (hyperplane in multidimensional space) $f(x) = x^Tw + b = 0$ returning $\sign(f(x))$
\ei

Loss functions:
\be
\i Logistic function
\bi 
\i penalizes points on correct side
\i close to decision boundary
\i asymptotical linear growth
\i MLE applied to logistic function $\log (1 + \exp(-t))$ is logistic regression
\i Logistic regression: $f(x) = \log(1 + \exp(-y \cdot (x^Tw + b)))$ 
\ei
\i Hinge loss: $f(x) = \max(0, 1 - t)$
\i Squared hinge loss: $f(x) = \max(0, 1 - t)^2$
\ee

\subsubsection{Support Vector Machine (SVM)}

Pick hyperplane with largest margin between classes.

\be
\i Hard margin SVM: No misclassified data points \[
\begin{array}{ll}
\min_{w \in \RS^d,b \in \RS} & \frac{1}{2} \norm{w}_2^2 \\
st & y^{(i)} \cdot (x^{(i)T}w + b) \geq 1
\end{array}
\]
\i Soft margin SVM: Allow errors \[
\begin{array}{ll}
\min_{w \in \RS^d,b \in \RS, \xi \in \RS^n} & \frac{1}{2} \norm{w}_2^2 + \frac{C}{n} \dsumni \xi_i \\
st & y^{(i)} \cdot ((x^{(i)}^T)w + b) \geq 1 - \xi_i \qquad , \xi_i \geq 0
\end{array}
\]
This would be with e.g. the hinge loss (minimize regularizer + empirical risk $\to$ RRM):
\[
\min_{w \in \RS^d,b \in \RS} \frac{1}{2} \norm{w}_2^2 + \frac{C}{n} \dsumni \max(0, 1 - y^{(i)} \cdot (x^{(i)T}w + b))
\]
\ee

Duality: If Primal problem convex and some conditions satisfied, optimal solution is equal to dual problem's solution (Strong duality).

In dual problems we only need scalar products of data points.

\be
\setcounter{enumi}{2}
\i Dual SVM: \[
\begin{array}{ll}
\max_{\alpha \in \RS^n} & \dsumni \alpha_i - \dsumni \dsumnj \alpha_i \alpha_j y^{(i)} y^{(j)} (x^{(i)})^T x^{(j)} \\
st & \dsumni \alpha_i y^{(i)} = 0 \\
& 0 \leq \alpha_i \leq \frac{C}{n}
\end{array}
\]
Used when many dimensions and hyperplane separator not linear and kernels are used.
\ee

\subsubsection{Kernels}

\bi
\i $x \in \RS^d$
\i map to $R^m$ using non-linear feature map $\phi$
\i Use linear SVM in $R^m$
\i never compute $\phi(x)$, only scalar products
\i use kernel function $k(x^{(i)}, x^{(j)})$
\ei


$k(a,b)$ is a kernel function... 
\bi 
\i If $\phi$ exists for $k(a,b) = \phi(a)\phi(b)$ for $a,b \in \RS$
\i If $\phi$ exists for $K = \phi(X)\phi(X)^T$ with data matrix $X$
\i Iff K is positive semidefinite ($K \succcurlyeq 0$) for any $n$ input points 
\bi
\i Sometimes $\phi$ maps to $\RS^\infty$ (Reproducing Kernel Hilbert Space)
\ei
\ei

Examples
\bi
\i Linear kernel for $k: \RS^d \times \RS^d \to \RS$ \[k(x^{(i)}, x^{(j)}) = (x^{(i)})^T x^{(j)}\]
\i Cosine kernel: Assume data have all norm 1: $\norm{x^{(i)}}_2 = 1$, angle measures similarity
\[
\cos(x^{(i)}, x^{(j)}) = \frac{(x^{(i)})^T x^{(j)}}{\norm{x^{(i)}} \norm{x^{(j)}}} = (x^{(i)})^T x^{(j)}
\]
\i Gaussian kernel (Radial basis function) \[
k(x^{(i)}, x^{(j)}) = \exp\left(-\frac{\norm{x^{(i)} - x^{(j)}}_2^2}{2\sigma^2}\right)
\]
maps to infinite dimensions
\i Polynomial Kernel for $c > 0$ and $s \in \NS$:
\[
k(x^{(i)}, x^{(j)}) = (x^{(i)})^T x^{(j)} + c)^s
\]
\i Sum and positive scalar product of kernels are kernels
\ei
\newpage
Kernel for Regularized Risk Minimization if norm from 2:
\bi
\i $\min_w L(w,X,y) + \lambda R(w)$
\i iff $R = h(\norm{w}_2)$ for non-decreasing $h$, then $w^* = \dsum_i \beta_i \phi(x^{(i)})$
\i LSR: $\min_w \norm{Xw-y}^2_2 \Rightarrow \min_\beta \norm{K\beta - y}^2_2$ 
\i Ridge Regression: $\min_w \norm{Xw-y}^2_2 + \lambda \norm{w}_2^2 \Rightarrow \min_\beta \norm{K\beta - y}^2_2 + \lambda \beta^T K \beta$
\i Regularized Logistic Regression, etc.
\ei

\subsection{Parametric vs Nonparametric Models}

\bi
\i Parametric if model depends on fixed number of parameters
\i Nonparametric e.g. SVM with Gaussian kernel
\ei

\subsection{Multiclass Classification}

\bi
\i One-vs-Rest: Train binary classifier for each class-pair, predict highest score class
\i One-vs-One: Train binary classifier for each class-pair, predict class with most votes
\i Direct methods: e.g. K-nearest neighbor 
\ei 

\subsubsection{K-nearest neighbor}

\bi
\i Find $k$ nearest neighbors in training data
\i Predict class with most votes
\i Choose $k$ with cross-validation
\i Small k $\to$ overfitting, large k $\to$ underfitting
\i Advantage: Easy, simple, no training phase, non-parametric
\i Disadvantage: Computationally expensive, sensitive to Noise
\ei

Regression: Compute average of $k$ nearest neighbors \[y = \frac{1}{k} \dsum_{i \in S} y^{(i)}\] or weighted \[y = \frac{1}{k} \dsum_{i \in S} \frac{1}{\norm{x-x^{(i)}}} \cdot y^{(i)}\]

\subsubsection{Naive Bayes}

\bi
\i Probabilistic ML algorithm
\i Assume features are conditionally independent
\i Compute $p(y \mid x) = \frac{p(x \mid y) \cdot p(y)}{p(x)}$
\i $p(y \mid x)$ is event $y$ in class $x$
\i Bayesian theorem: $p(y \mid x) = \frac{p(x \mid y) \cdot p(y)}{p(x)}$
\i Laplacian Smoothing for zero values: e.g. with words $w_i$ of class $x$ \[
p(w_i \mid x) = \frac{\text{frequency of $w_i$} +1 }{\text{words in $x$} + \text{total unique words}}
\]
\ei

\subsubsection{Decision Trees}

\bi
\i $X = \RS^d$
\i $Y = {1,2,...,m}$
\i Recursive partitioning of data
\i Split data into subsets
\i Choose feature and threshold to split (entropy, gini index)
\i Repeat until stopping criterion
\i Predict class with majority vote
\i Advantage:
\i Disadvantage: Overfitting, sensitive to noise
\ei

\subsubsection{Ensemble Learning}

\bi
\i Combine multiple models to improve performance
\i Bagging: Train multiple models on random subsets of data, smaller variance
\i Bootstrapping: Randomly sample data with replacement
\i Boosting: Train multiple models sequentially, each model corrects previous model
\ei

\subsubsubsection{Random Forest}

\bi
\i Bagging with decision trees
\i Each decision tree trained on random bootstrap subset of size $m$ of data of size $n$
\i Construct decision tree $T_b$:
\bi
\i For each node that contains more than $n_{min}$ data points:
\i Randomly select $p$ of $d$ features
\i Split node with best feature and threshold
\i Repeat until all nodes smaller
\ei
\i Output: Random decision trees $T_1, T_2, \ldots, T_B$
\ei

\subsection{Generative Models}

\bi
\i Discrimative: Learn $p(y \mid x)$
\i Generative: Learn $p(y \mid x) \cdot p(x) = p(x, y)$ joint probability
\ei

\subsubsection{Discriminant Analysis}

\subsubsection{Gaussian Discriminant Analysis}

\bi
\i generate new data, also for classification
\i $\mu = \frac{1}{n} \dsum_i x^{(i)}$
\i $\Sigma = \frac{1}{n} \dsum_i (x^{(i)} - \mu) (x^{(i)} - \mu)^T$
\i for each class, get $\mu_c$ and $\Sigma_c$
\i estimate $p(x,y) = p(x \mid y) \cdot p(y)$ with $p(y=c) = \frac{n_c}{n}$ where $n_c$ is number of data points in class $c$ and $N$ is total amount of data points
\i this becomes $x^T \Sigma x + v^Tx + t=0$
\i Boundary is quadratic
\i number of parameters to be estimated: $2 \frac{d(d+1)}{2} + 2d$
(much higher than for discriminative models)
\ei

\subsubsection{Linear Discriminant Analysis}

\bi
\i Assume $\Sigma = \Sigma_{-1} = \Sigma_{+1}$ for all classes
\i number of parameters to be estimated: $\frac{d(d+1)}{2} + 2d$
\ei

\section{Supervised learning}

\bi
\i predict label for $(x^{(i)})_i$
\i find structure, possibly assign labels
\i can generate new data
\ei

\subsection{k-means Objective}

\bi
\i for $x^{(i)} \in \RS^d$ assume given data points $(x^{(i)})^n_{i=1}$
\i group into clusters $C = \{C_1, C_2, \ldots, C_k\}$
\i construct class $k$ class centers $\mu_i$ representing groups
\i Find centers such that sum of squared distances of data points to closest center is minimized: \[
\min_{\mu_1, \ldots, \mu_k \in \RS^d, C} \dsum_{j=1}^k \dsum_{x^{(i)} \in C_j} \norm{x^{(i)} - \mu_j}^2
\]
\i non-convex min. problem
\ei

%\[
%\begin{array}{ll}
%\min_w \frac{1}{2} \norm{Xw-y}_2^2 + \lambda \norm{w}_2^2 & \text{Ridge regression}\\\\
%\min_w \frac{1}{2} \norm{Xw-y}_2^2 + \lambda \norm{w}_1 & \text{Lasso regression}
%\end{array}
%\]