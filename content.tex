\npsection{Introduction}

ML: Tries to automate the process of \bx{inductive inference}.

\be
\i Deduction: Learning from rules
\i Induction: Learning from examples
\ee

\subsection{Math}

\TODO{norms}
\TODO{determinant, trace, inverse}
\TODO{eigenvalues + eigenvectors}
\TODO{semidefinite, definite, indefinite}
\TODO{linear eq}
\TODO{inverse proof}

\subsubsection{Matrix calculus}

\newcommand{\dlw}[2]{\frac{\partial L_#1}{\partial w_#2}}
\begin{align*}
L(w): \RS \to \RS^m
\Rightarrow  \nabla L(w) = \vct{\drv{w_1}L(w)\\\vdots\\\drv{w_n}L(w)}
\Rightarrow \nabla L(w) = \left(\begin{array}{cccc}
\dlw{1}{1} & \dlw{2}{1} & \ldots & \dlw{m}{1} \\
\dlw{1}{2} & \dlw{2}{2} & \ldots & \dlw{m}{2} \\
\vdots & \vdots & \ddots & \vdots \\
\dlw{1}{d} & \dlw{2}{d} & \ldots & \dlw{m}{d}
\end{array}\right)
\end{align*}

\npsection{Supervised learning}

\bi
\i input $X$, output $Y$
\i training data: $(x^{(i)}, y^{(i)})_{i=1..n} \subset X \times Y$
\i Goal: learn $f: X \to Y$ for model class $F$ on examples
\ei

\subsection{Least squares regression}
$\tilde X, \tilde w$ are extended with bias:
\begin{align*}
\min_{\tilde w} \frac{1}{2} \norm{\tilde X \tilde w - y}^2 \Rightarrow \min_w \frac{1}{2} \norm{Xw-y}^2
\end{align*}
Solve with gradient and set to zero:
\begin{align*}
L &= \frac{1}{2} \dsum_{i=1}^n ((X_i^Tw_i)-y_i)^2 \\
&= \frac{1}{2} \left(\dsum_{i=1}^n (X_i^Tw_i)^2 - 2(X_i^Tw_i)y_i + y_i^2 \right) \\
\end{align*}
\begin{align*}
  \nabla L &= \drv{w} \left(  \frac{1}{2} \left(\dsum_{i=1}^n (X_i^Tw_i)^2 - 2(X_i^Tw_i)y_i + y_i^2 \right) \right)\\
  &= \frac{1}{2} \left(\dsum_{i=1}^n 2(X_i^TX_iw_i) - 2(X_i^T)y_i \right)\\
  &= \dsum_{i=1}^n X_i^TX_iw_i - X_i^Ty_i\\
  &= X^TXw - X^Ty\\
  &= X^T(Xw-y)
\end{align*}
\begin{align*}
\nabla L = X^T(Xw-y) = 0 \Rightarrow (X^TX)w = X^Ty \Rightarrow w = (X^TX)^{-1}X^Ty
\end{align*}

\subsection{Gradient descent}
Alternative to least squares regression. Algorithm:
\be
\i Compute gradient $\nabla L(w) = X^T(Xw-y)$
\i Negative gradient shows to steepest descent
\i $w^{(t+1)} = w^{(t)} - \gamma^{(t)} \cdot \nabla L(w^{(t)})$
\ee

\subsection{Derivative examples}

\bi
\i $L(w) = w_1^2 + w_2^2 \\\Rightarrow \nabla L(w) = \vct{2w_1\\2w_2}$
\i $L(w) = \norm{w} ^2_2 = w^Tw \\\Rightarrow \nabla L(w) = 2w$
\i $L(w) = w^TAw \\\Rightarrow \nabla L(w) = Aw + A^Tw$
\i $L(w) = \norm{Xw-y}^2 = w^TX^TXw - y^TXw - w^TX^Ty + y^Ty \\\Rightarrow \nabla L(w) = 2X^T(Xw - y)$
\ei

\subsection{Convexity}

Set $C$ convex if line between any two points of $C$ in $C$. $\forall x,y \in C$ and $\lambda \in \RS$ with $0 \leq \lambda \leq 1$: \[\lambda x + (1 - \lambda) y \in C\]

Function $f: \RS^d \to \RS$ convex if $\dom(f)$ is a convex set and $\forall x,y \in \dom(f)$, $\lambda \in \RS$ with $0 \leq \lambda \leq 1$: \[f(\lambda x + (1 - \lambda) y) \leq \lambda f(x) + (1 - \lambda) f(y)\]

Gradient descent returns global optimum for convex functions.
